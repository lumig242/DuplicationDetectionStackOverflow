{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Report: Question Predictor for StackOverflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "StackOverflow is the largest online QA platform for programmers to ask, answer various questions and learn new knowledge and technique. By the end of August 2015, there are more than 10,000,000 questions posted on Stack Overflow. However, As the number of questions grows, we may find that there are more and more questions that focuses on the same topic or features, or actually have the same answers. We define these questions as duplicate questions. Duplicate questions will waste the machine resources and made question raiser to wait for a long time for a queston that has been answered already. Currently, Stack Overflow encourage users to manually mark any potential duplicate questions, which is not efficient and may have some bias. Here is an example of duplicate questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](image01.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack Overflow **does not** have a standard method or label to distinguish duplicate questions from normal questions. However, Stack Overflow users usually have default measures to signify possible duplicate questions. We use three conditions to decide whether a question is a duplicate question or not:\n",
    "\n",
    "1. Check where **[duplicate]** appears in the title.\n",
    "2. Check whether [**This question already has an answer**] or  in the body of the questions.\n",
    "3. Check whether [**Possible duplicate**] in the body of the questions.\n",
    "\n",
    "A question satisfying any of the forementioned conditions will be classified as duplicate questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got our data from [here](https://archive.org/details/stackexchange) [1].\n",
    "\n",
    "\n",
    "The original date is in XML format. We need to keep only all questions(PostTypeId=1) and remove invalid questions(Id=-1). The following code is used to extract useful information from data. For duplicate questions, we also extract the id of the existing questions for further analysis.\n",
    "\n",
    "#### The basic statistic of the data is listed in the following:\n",
    "\n",
    "Total number of records: 32209819\n",
    "\n",
    "\n",
    "Total number of valid questions: 12350818\n",
    "\n",
    "Total number of duplicate questions: 48197\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "\n",
    "\n",
    "def data_parser():\n",
    "    regex = re.compile('([a-z0-9]+)=\"([^\"]+)\"', re.I)\n",
    "    dup_id = re.compile(\"stackoverflow.com/questions/(\\d+)/\")\n",
    "\n",
    "    fp_all = open(\"all_questions\", \"w\")\n",
    "    fp_dup = open(\"dup_questions\", \"w\")\n",
    "    with open(sys.argv[1], \"r\") as fp:\n",
    "        for line in fp:\n",
    "            try:\n",
    "                line = line.strip()\n",
    "                post = dict(re.findall(regex, line))\n",
    "                if \"Id\" not in post:\n",
    "                    continue\n",
    "\n",
    "                if \"has already been answered\" in post[\"Body\"] or (\"Title\" in post and \"[duplicate]\" in post[\"Title\"]):\n",
    "                    post[\"dups\"] = [int(num) for num in re.findall(dup_id, post[\"Body\"])]\n",
    "                    fp_dup.write(post[\"Id\"] + \"wcyz666SQL\" + json.dumps(post) + \"\\n\")\n",
    "\n",
    "                fp_all.write(post[\"Id\"] + \"wcyz666SQL\" + json.dumps(post) + \"\\n\")\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the data to **MySQL database** (deployed on Amazon AWS) for future retrival.\n",
    "\n",
    "CREATE TABLE `all_questions` (`id` bigint(64), `text` text charset 'utf8mb4' collate utf8mb4_unicode_ci) ENGINE=MyISAM DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;\n",
    "\n",
    "CREATE TABLE `dup_questions` (`id` bigint(64), `text` text charset 'utf8mb4' collate utf8mb4_unicode_ci) ENGINE=MyISAM DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;\n",
    "\n",
    "load data local infile '/home/ubuntu/all_questions' into table all_questions character set 'utf8mb4' fields terminated by \"wcyz666SQL\";\n",
    "\n",
    "load data local infile '/home/ubuntu/dup_questions' into table dup_questions character set 'utf8mb4' fields terminated by \"wcyz666SQL\";\n",
    "\n",
    "create INDEX pds_index1 on all_questions(id);\n",
    "\n",
    "create INDEX pds_index2 on dup_questions(id);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain duplicate pairs, we need to insert more questions into MySQL database. In the previous step, the questions that has been marked as **duplicate questions** will has links (enclosing question ID inside) to source questions, which we should also insert to form duplicate pairs. \n",
    "\n",
    "There are 1998 duplicate questions marked but have no external link to other questions. These questions will be eliminated in this step. Other questions will be iterated one-by-one and source questions will be selected from *all_questions* and then inserted to *dup_questions*. Related code is shown in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import MySQLdb\n",
    "\n",
    "def data_clean():\n",
    "    fp_dup = open(\"duped_questions\", \"w\")\n",
    "    cnx = MySQLdb.connect(user='root', password='123456',\n",
    "                                  host='ec2-52-91-216-34.compute-1.amazonaws.com',\n",
    "                                  database='pds_team')\n",
    "\n",
    "    fp_dup = open(\"duped_questions\", \"w\")\n",
    "    cnx = MySQLdb.connect(user='root', password='',\n",
    "                                  host='ec2-52-91-216-34.compute-1.amazonaws.com',\n",
    "                                  database='pds_team')\n",
    "\n",
    "    all_questions = MySQLdb.connect(user='root', password='',\n",
    "                                  host='ec2-52-91-216-34.compute-1.amazonaws.com',\n",
    "                                  database='pds_team')\n",
    "    no_dup = []\n",
    "    cnx.query(\"SELECT * FROM dup_questions\")\n",
    "    res = cnx.use_result()\n",
    "    count = 0\n",
    "    while True:\n",
    "        result = res.fetch_row(maxrows=100)\n",
    "        if len(result) == 0:\n",
    "            break\n",
    "        count += 100\n",
    "        print str(count) + \" items is completed\"\n",
    "        for id, post_json in result:\n",
    "            try:\n",
    "                post = json.loads(post_json)\n",
    "                if len(post['dups']) == 0:\n",
    "                    no_dup.append(id)\n",
    "                else:\n",
    "                    for dup_id in post[\"dups\"]:\n",
    "                        all_questions.query(\"SELECT * FROM all_questions WHERE id=\" + str(dup_id))\n",
    "                        all_res = all_questions.store_result()\n",
    "                        for _, item in all_res.fetch_row():\n",
    "\n",
    "                            fp_dup.write(str(dup_id) + \"wcyz666SQL\" + item + \"\\n\")\n",
    "            except:\n",
    "                pass\n",
    "    fp_dup.close()\n",
    "    cnx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "To evaluate the similarity between two questions, we applied different indicators as features, including title, questions body, question topic tags. Besides, we also applied the LDA model to evaluate the similarity on topics between the questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "\n",
    "We preprocess all the texts in the question title/content by removing punctuations, stemming, tokenizing etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_with_preprocess(text):\n",
    "    \"\"\"\n",
    "    Tokenize with preprocessing. Remove all punctuations and apply stemming on the tokens\n",
    "    :param text: String\n",
    "    :return: list of tokens\n",
    "    \"\"\"\n",
    "    return map(__stemmer.stem, filter(lambda w: w not in stop,\n",
    "                                        nltk.word_tokenize(re.sub(_punc_pattern, '', text.lower()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text similarity\n",
    "We try both cosine and jacard similarity as the similarity indicator, based on tfidf/ token set respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    :param text1, text2: list of tokens\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    return 1.0 - nltk.jaccard_distance(set(text1), set(text2))\n",
    "\n",
    "def cosine_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    :param text1, text2: list of tokens\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    tfidf = TfidfVectorizer().fit_transform(map(_SimilarityUtil._inverse, [text1, text2]))\n",
    "    return 1 - cosine(tfidf[0].todense(), tfidf[1].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA model\n",
    "We applied LDA on the train set to model the topics among each questions. During evaluating, we use the pretrained lda model to tranform the text and get a distribution(a vetor of topic_size) among all the topics. We compute the euclidean distance of the two distribution vector to get the feature.\n",
    "\n",
    "As the size of the dataset is rather large(with over 10k samples and an average length of over 500 words), we failed to train the lda model based on all text materials. As a compromise, we choose the most common 1000 words(already stemmed and preprocessed by the function above) as the base dictionary in lda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LDA(object):\n",
    "    def __init__(self, corpus, load=False, n_topic=3):\n",
    "        self.words = np.array([t[0] for t in Counter(itertools.chain(*corpus)).most_common(1000)])\n",
    "        X = np.array(map(self._transform, corpus))\n",
    "        self._model = lda.LDA(n_topics=n_topic, random_state=0, n_iter=100)\n",
    "        self._model.fit(X)\n",
    "    \n",
    "    def get_topic(self, tokens):\n",
    "        return self._model.transform(self._transform(tokens))\n",
    "    \n",
    "    # more functions to go.............................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet of code shows how we train the model. The body of all questions in the train_set are fed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lda_model(train_set):\n",
    "    lda = LDA([q['body'] for q in train_set.values()], n_topic=20)\n",
    "    lda.print_topic_top_words(n_top_words=8)\n",
    "    return lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine them together\n",
    "For each question, it has a list of attributes. With experiment, we decide to choose \"title\", \"body\", \"tag\" and the \"topic\" inferred by the lda model above as separate features. As a result, in the final implemenation of our model, the similarity(the possibility of duplicate) of each pair of questions are determined by a 4-dimension vector.\n",
    "\n",
    "Here is how we extract the information of each question from the raw dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose preprocessing for tokenize\n",
    "tokenize = tokenize_with_preprocess\n",
    "\n",
    "def _stripe_tag(tags):\n",
    "    return sorted(tags.replace('&lt', '').replace('&gt;', ' ').split())\n",
    "\n",
    "def preprocess(q_set):\n",
    "    return {j['Id']: {'id': j['Id'],\n",
    "                      'title': tokenize(j['Title']),\n",
    "                      'body': tokenize(j['Body']),\n",
    "                      'tags': _stripe_tag(j['Tags'])} for j in q_set}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described above in this section, we are able to build a feature vector for each pair of questions in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_feature(q1, q2, lda):\n",
    "    \"\"\"\n",
    "    :param q1, q2: Pre-processed questions. A dict of all required attributes\n",
    "    :param lda: Pre-trained lda model\n",
    "    :return: A 4-d array of feature \n",
    "    \"\"\"\n",
    "    # Title similarity\n",
    "    x0 = SimilarityUtil.cosine_similarity(q1['title'], q2['title'])\n",
    "    x1 = SimilarityUtil.cosine_similarity(q1['body'], q2['body'])\n",
    "    # x2 = SimilarityUtil.jaccard_similarity(q1['tags'], q2['tags'])\n",
    "    x2 = SimilarityUtil.cosine_similarity(q1['tags'], q2['tags'])\n",
    "    x3 = np.linalg.norm(lda.get_topic(q1['body']) - lda.get_topic(q2['body']))\n",
    "    return np.array([x0, x1, x2, x3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Validate\n",
    "An interesting thing in this problem is that the actual train/test sample is not a single question itself, but the relationship between to questions. It is feasible to build and validate our model by spliting all the relationships(pairs) but we found some problems.\n",
    "\n",
    "1. With around 10k questions, the algorithm will yield over 10^8 pair which is a disaster for the model to build.\n",
    "2. Even if we have a powerful machine, the model will be extremly biased as only 1/10k of the pairs are positive samples.\n",
    "3. The model cannot be applied to discovering duplicate questions easily.\n",
    "\n",
    "As a result, we decided to find another to build the model and it will be even better if the model cna be applied in to real world use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Test Set\n",
    "Considering the real world use case, we split the question set into 70% vs 30% as old questions and new questions. Old questions are regarded as those already been posted in the forum and are used in the training phase to build our model. New questions are known as the questions that are posted after the build of the model and thus we will use the model to find/validate the duplicate questions within new question sets.\n",
    "\n",
    "Later, our model will be validate both on if a new question is duplicate from an old question and if a new question is duplicate from another new question, which matches the real world use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load():\n",
    "    all_questions = QuestionSet('ec2-52-91-216-34.compute-1.amazonaws.com', 'root', '123456', 'pds_team')\n",
    "    qids = all_questions.keys()\n",
    "    random.shuffle(qids)\n",
    "    split_size = int(len(qids) * 0.7)\n",
    "\n",
    "    print \"split train test sets\"\n",
    "    train_set = preprocess(all_questions[qid] for qid in qids[:split_size])\n",
    "    test_set = preprocess(all_questions[qid] for qid in qids[split_size:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preparation of training set is quite straight forward. We picked all the duplicated question pairs in the old questions as the training data. To keep the training dataset unbias, we randomly pick pairs with comparable amount as the negative sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_train_data(all_questions, train_set, lda):\n",
    "    X = []\n",
    "    y = []\n",
    "    for q in train_set:\n",
    "        for dup in all_questions.get_dup(q):\n",
    "            if dup in train_set:\n",
    "                X.append(build_feature(train_set[q], train_set[dup], lda))\n",
    "                y.append(1)\n",
    "    print 'Generating random negative samples'\n",
    "    # Generate negative samples as the same scale\n",
    "    size_positive = len(y)\n",
    "    q_set = train_set.keys()\n",
    "    for _ in xrange(size_positive):\n",
    "        q1 = random.choice(q_set)\n",
    "        q2 = random.choice(q_set)\n",
    "        X.append(build_feature(train_set[q1], train_set[q2], lda))\n",
    "        y.append(0 if q2 not in all_questions.get_dup(q1) else 1)\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preparation of test data is almost the same, except that we explicitly distinguish the relationship of old-new from new-new question pairs, as discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_validation_data(all_questions, train_set, test_set, lda):\n",
    "    \"\"\"\n",
    "    :param all_questions:\n",
    "    :param test_set:\n",
    "    :param lda:\n",
    "    :return: A tuple of four\n",
    "     X_test_cross: feature matrix between newly found questions and old qustions\n",
    "     y_test_cross: label array .................................................\n",
    "     X_test_new:   feature matrix among newly found questions\n",
    "     y_test_new:   label array ..............................\n",
    "    \"\"\"\n",
    "    X_test_cross, y_test_cross = [], []\n",
    "    X_test_new, y_test_new = [], []\n",
    "    for q in test_set:\n",
    "        for dup in all_questions.get_dup(q):\n",
    "            if dup in test_set:\n",
    "                # Among new questions\n",
    "                X_test_new.append(build_feature(test_set[q], test_set[dup], lda))\n",
    "                y_test_new.append(1)\n",
    "            elif dup in train_set:\n",
    "                X_test_cross.append(build_feature(test_set[q], train_set[dup], lda))\n",
    "                y_test_cross.append(1)\n",
    "\n",
    "    # Generate negative samples as the same scale\n",
    "    print 'Generating random negative samples'\n",
    "    size_positive_cross = len(y_test_cross)\n",
    "    size_positive_new = len(y_test_new)\n",
    "    q_set_train = train_set.keys()\n",
    "    q_set_test = test_set.keys()\n",
    "\n",
    "    for _ in xrange(20*size_positive_cross):\n",
    "        q1 = random.choice(q_set_train)\n",
    "        q2 = random.choice(q_set_test)\n",
    "        X_test_cross.append(build_feature(train_set[q1], test_set[q2], lda))\n",
    "        y_test_cross.append(0 if q2 not in all_questions.get_dup(q1) else 1)\n",
    "\n",
    "    for _ in xrange(size_positive_new):\n",
    "        q1 = random.choice(q_set_test)\n",
    "        q2 = random.choice(q_set_test)\n",
    "        X_test_new.append(build_feature(test_set[q1], test_set[q2], lda))\n",
    "        y_test_new.append(0 if q2 not in all_questions.get_dup(q1) else 1)\n",
    "\n",
    "    return tuple(map(np.array, (X_test_cross, y_test_cross, X_test_new, y_test_new)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we end up with `X_train` `y_train` as the training data and two pairs of test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load():\n",
    "    # .......................\n",
    "    \n",
    "    print 'loading training set'\n",
    "    X_train, y_train = prepare_train_data(all_questions, train_set, lda)\n",
    "    print 'loading test set'\n",
    "    X_test_cross, y_test_cross, X_test_new, y_test_new = \\\n",
    "        prepare_validation_data(all_questions, train_set, test_set, lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that in the above code snippet, even in the test phase, the negative pairs vs positive pairs are among 1:1, which is not true in real word. To further ensure the usability, we generate three test set with different negative sample weight of 1,5,20, which means in the validation dataset, the duplicate vs non-duplicate pairs are 1:1, 1:5, 1:20 respectively. We found that if we increase the weight further, say 40, 50, 100 etc, it won't affect the performance of our model significantly. As a result we use the performance of these three weight scale as the metrics of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the feature vector only length 4, we choose the simple logistic regression which provides comparable performance as other more sophiscated ones but is much easier to be explained and understand in real word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ec324cf6caa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlogistic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "logistic = linear_model.LogisticRegression(class_weight={0:5, 1:1})\n",
    "lr = logistic.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print classification_report(lr.predict(X_test_cross), y_test_cross)\n",
    "print classification_report(lr.predict(X_test_new), y_test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.99      0.99      0.99    402188\n",
    "          1       0.80      0.82      0.81     19597\n",
    "\n",
    "avg / total       0.98      0.98      0.98    421785\n",
    "\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.99      0.99      0.99    173388\n",
    "          1       0.81      0.82      0.82      8514\n",
    "\n",
    "avg / total       0.98      0.98      0.98    181902\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't do lots of param grid search work apart from simply set the class weight to add the penalty for true-negative errors(duplicate pairs predicted as non-duplicate) since our point is to discover all the potential duplicate questions. The most meaningful number we care about in the report is the recall rate of the duplicate questions(marked as 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall rate\n",
    "With a 20 times negative weight, we are able to achieve slightly over 80% recall rate with a comparable precision rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](result1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coef of each feature\n",
    "\n",
    "The coefficiency describes to what extent a feature contirbutes to the prediction. Still we found the similarity of body text and their topic is the determining feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coef_ = {'title': 16.12967728, 'body': 19.89534911, 'tag': 7.91410006, 'topic': 19.8966333}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discover new unmarked duplicate pairs\n",
    "\n",
    "Finally we will apply our model into reality. We randomly choose 10000 questions from *all_questions*, and use the classifier to see if there is any potential duplicate pairs that are not marked as duplicate pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main_discover_new():\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    with open('final-data20.pkl', 'rb') as f:\n",
    "        res = pickle.load(f)\n",
    "    with open('all_questions.pkl', 'rb') as f:\n",
    "        all = pickle.load(f)\n",
    "        \n",
    "    print \"train data loaded\"\n",
    "    \n",
    "    res = [np.nan_to_num(a) for a in res]\n",
    "\n",
    "    X_train, y_train, _, _, _, _ = res\n",
    "    logistic = linear_model.LogisticRegression(class_weight={0:1, 1:1})\n",
    "    lr = logistic.fit(X_train, y_train)\n",
    "    print \"trained data ready\"\n",
    "\n",
    "    with open('lda.pkl', 'rb') as f:\n",
    "        lda = pickle.load(f)\n",
    "    print \"lda data loaded\"\n",
    "\n",
    "    random_questions = get_random(10000, all)\n",
    "    ids = list(random_questions.keys())\n",
    "    for id_1 in ids:\n",
    "        for id_2 in ids:\n",
    "            if id_1 != id_2:\n",
    "                try:\n",
    "                    feature = build_feature(random_questions[id_1], random_questions[id_2], lda)\n",
    "                    if lr.predict(feature)[0] == 1:\n",
    "                        print id_1, id_2\n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The running result is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "11200069 1499950\n",
    "11200069 14014974\n",
    "11200069 1248100\n",
    "11200069 14042658\n",
    "11200069 635297\n",
    "9976291 3622267\n",
    "9976291 6006276\n",
    "9976291 12203403\n",
    "9976291 11597222\n",
    "9976291 12489473\n",
    "9976291 9461709\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatly, we can still find some mis-classified duplicate pairs. Even more unfortunatly, for instance, the first pair: The content of question 11200069 is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](image02.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the second question 1499950 is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](image03.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see then have similar tags, title and topics and similar content, but they are talking about different tech details and should not be marked as duplicate.\n",
    "\n",
    "This is obviously a limitation of our model. Even if two questions have the same tags, same topics and similar content, they are not necessarily to be duplicate, since two unique questions might be asked on exactly the same problem/code/api etc. and it is extremly difficult for algorithms to differentiate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this project, we applied machine learning related technique to discover potential duplicate questions in Stackoverflow. We collect the raw XML-formatted data from Stackoverlow archive and stored the clean data into MySQL. Then We extract four features, namely, **title, topic, post and tag**, calculate the distance among them using LDA model and cosine/jaccard similarity. After that, we partition our dataset, and train our model using 70% of the original dataset using **logistic regression**. Finally we validate our model using the validation set, and tune some parameters to achieve the best precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reference\n",
    "\n",
    "1. https://archive.org/details/stackexchange\n",
    "2. Zhang Y, Lo D, Xia X et al. Multi-factor duplicate question detection in Stack Overflow. JOURNAL OF COMPUTER\n",
    "SCIENCE AND TECHNOLOGY 30(5): 981â€“997 Sept. 2015. DOI 10.1007/s11390-015-1576-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A\n",
    "Our full source code is available at: https://github.com/lumig242/DuplicationDetectionStackOverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
