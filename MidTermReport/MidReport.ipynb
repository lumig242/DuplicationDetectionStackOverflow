{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm Report: Question Predictor for StackOverflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "StackOverflow is the largest online QA platform for programmers to ask, answer or learn new knowledge and technique. By the end of August 2015, there are more than 10,000,000 questions posted on Stack Overflow. However, As the number of questions grows, we may find that there are more and more questions that focuses on the same topic or features, or actually have the same answers. We define these questions as duplicate questions. Duplicate questions will waste the machine resources and made question raiser to wait for a long time for a queston that has been answered already. Currently, Stack Overflow encourage users to manually mark any potential duplicate questions, which is not efficient and may have some bias. Here is an example of duplicate questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](image01.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack Overflow **does not** have a standard method or label to distinguish duplicate questions from normal questions. However, Stack Overflow users usually have default measures to signify possible duplicate questions. We use three conditions to decide whether a question is a duplicate question or not:\n",
    "\n",
    "1. Check where **[duplicate]** appears in the title.\n",
    "2. Check whether **This question already has an answer** in the body of the questions.\n",
    "3. Check whether **Possible duplicate** in the body of the questions.\n",
    "\n",
    "A question satisfying any of the forementioned conditions will be classified as duplicate questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got our data from [here](https://archive.org/details/stackexchange).\n",
    "\n",
    "\n",
    "The original date is in XML format. We need to keep only all questions(PostTypeId=1) and remove invalid questions(Id=-1). The following code is used to extract useful information from data. For duplicate questions, we also extract the id of the existing questions for further analysis.\n",
    "\n",
    "#### The basic statistic of the data is listed in the following:\n",
    "\n",
    "Total number of records: 32209819\n",
    "\n",
    "\n",
    "Total number of valid questions: 12350818\n",
    "\n",
    "Total number of duplicate questions: 48197\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mIOError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3aede8f70e00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mfp_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"all_questions\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mfp_dup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dup_questions\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: '-f'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "\n",
    "regex = re.compile('([a-z0-9]+)=\"([^\"]+)\"', re.I)\n",
    "dup_id = re.compile(\"stackoverflow.com/questions/(\\d+)/\")\n",
    "\n",
    "fp_all = open(\"all_questions\", \"w\")\n",
    "fp_dup = open(\"dup_questions\", \"w\")\n",
    "with open(sys.argv[1], \"r\") as fp:\n",
    "    for line in fp:\n",
    "        try:\n",
    "            line = line.strip()\n",
    "            post = dict(re.findall(regex, line))\n",
    "            if \"Id\" not in post:\n",
    "                continue\n",
    "\n",
    "            if \"has already been answered\" in post[\"Body\"] or (\"Title\" in post and \"[duplicate]\" in post[\"Title\"]):\n",
    "                post[\"dups\"] = [int(num) for num in re.findall(dup_id, post[\"Body\"])]\n",
    "                fp_dup.write(post[\"Id\"] + \"wcyz666SQL\" + json.dumps(post) + \"\\n\")\n",
    "\n",
    "            fp_all.write(post[\"Id\"] + \"wcyz666SQL\" + json.dumps(post) + \"\\n\")\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load the data to **MySQL database** for future retrival.\n",
    "\n",
    "CREATE TABLE `all_questions` (`id` bigint(64), `text` text charset 'utf8mb4' collate utf8mb4_unicode_ci) ENGINE=MyISAM DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;\n",
    "\n",
    "CREATE TABLE `dup_questions` (`id` bigint(64), `text` text charset 'utf8mb4' collate utf8mb4_unicode_ci) ENGINE=MyISAM DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;\n",
    "\n",
    "load data local infile '/home/ubuntu/all_questions' into table all_questions character set 'utf8mb4' fields terminated by \"wcyz666SQL\";\n",
    "\n",
    "load data local infile '/home/ubuntu/dup_questions' into table dup_questions character set 'utf8mb4' fields terminated by \"wcyz666SQL\";\n",
    "\n",
    "create INDEX pds_index1 on all_questions(id);\n",
    "\n",
    "create INDEX pds_index2 on dup_questions(id);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Build\n",
    "\n",
    "To evaluate the similarity between two questions, we applied different indicators as features, including title, questions body, question topic tags. Besides, we also applied the LDA model to evaluate the similarity on topics between the questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "\n",
    "We preprocess all the texts in the question title/content by removing punctuations, stemming, tokenizing etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_with_preprocess(text):\n",
    "    \"\"\"\n",
    "    Tokenize with preprocessing. Remove all punctuations and apply stemming on the tokens\n",
    "    :param text: String\n",
    "    :return: list of tokens\n",
    "    \"\"\"\n",
    "    return map(__stemmer.stem, filter(lambda w: w not in stop,\n",
    "                                        nltk.word_tokenize(re.sub(_punc_pattern, '', text.lower()))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text similarity\n",
    "We try both cosine and jacard similarity as the similarity indicator, based on tfidf/ token set respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    :param text1, text2: list of tokens\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    return 1.0 - nltk.jaccard_distance(set(text1), set(text2))\n",
    "\n",
    "def cosine_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    :param text1, text2: list of tokens\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    tfidf = TfidfVectorizer().fit_transform(map(_SimilarityUtil._inverse, [text1, text2]))\n",
    "    return 1 - cosine(tfidf[0].todense(), tfidf[1].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA model\n",
    "We applied LDA on the train set to model the topics among each questions. During evaluating, we use the pretrained lda model to tranform the text and get a distribution(a vetor of topic_size) among all the topics. We compute the euclidean distance of the two distribution vector to get the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LDA(object):\n",
    "    def __init__(self, corpus, load=False, n_topic=3):\n",
    "        self.words = np.array(list(set(itertools.chain(*corpus))))\n",
    "        X = np.array(map(self._transform, corpus))\n",
    "        self._model = lda.LDA(n_topics=n_topic, random_state=0, n_iter=100)\n",
    "        self._model.fit(X)\n",
    "    \n",
    "    def get_topic(self, tokens):\n",
    "        return self._model.transform(self._transform(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "We split the question set into 70% vs 30% as train/test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train our model based on current dataset and try to come up with a efficient duplicate queston indicator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reference\n",
    "\n",
    "Zhang Y, Lo D, Xia X et al. Multi-factor duplicate question detection in Stack Overflow. JOURNAL OF COMPUTER\n",
    "SCIENCE AND TECHNOLOGY 30(5): 981–997 Sept. 2015. DOI 10.1007/s11390-015-1576-4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
